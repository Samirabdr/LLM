{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "98OQ9E9Fffa_"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers datasets evaluate\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9wh822wGgh-M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "#Import IMDB \n",
        "\n",
        "from datasets import load_dataset\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "print(imdb_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 25000\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hps6rvDShGwz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'label': 0,\n",
            " 'text': 'It was great to see some of my favorite stars of 30 years ago '\n",
            "         'including John Ritter, Ben Gazarra and Audrey Hepburn. They looked '\n",
            "         'quite wonderful. But that was it. They were not given any characters '\n",
            "         'or good lines to work with. I neither understood or cared what the '\n",
            "         'characters were doing.<br /><br />Some of the smaller female roles '\n",
            "         'were fine, Patty Henson and Colleen Camp were quite competent and '\n",
            "         'confident in their small sidekick parts. They showed some talent and '\n",
            "         \"it is sad they didn't go on to star in more and better films. Sadly, \"\n",
            "         \"I didn't think Dorothy Stratten got a chance to act in this her only \"\n",
            "         'important film role.<br /><br />The film appears to have some fans, '\n",
            "         'and I was very open-minded when I started watching it. I am a big '\n",
            "         'Peter Bogdanovich fan and I enjoyed his last movie, \"Cat\\'s Meow\" '\n",
            "         'and all his early ones from \"Targets\" to \"Nickleodeon\". So, it '\n",
            "         'really surprised me that I was barely able to keep awake watching '\n",
            "         'this one.<br /><br />It is ironic that this movie is about a '\n",
            "         'detective agency where the detectives and clients get romantically '\n",
            "         \"involved with each other. Five years later, Bogdanovich's \"\n",
            "         'ex-girlfriend, Cybil Shepherd had a hit television series called '\n",
            "         '\"Moonlighting\" stealing the story idea from Bogdanovich. Of course, '\n",
            "         'there was a great difference in that the series relied on tons of '\n",
            "         'witty dialogue, while this tries to make do with slapstick and a few '\n",
            "         'screwball lines.<br /><br />Bottom line: It ain\\'t no \"Paper Moon\" '\n",
            "         'and only a very pale version of \"What\\'s Up, Doc\".'}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "pprint(imdb_dataset[\"train\"][10]) # Label 1 means positive review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KJ_f7Fefhgln"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#Each pre-trained model has its own tokenizer. If we give the model name, the corresponding tokenizer will be loaded.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A-4l-w4jifRa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 50000/50000 [00:13<00:00, 3610.77 examples/s]\n"
          ]
        }
      ],
      "source": [
        "#We will define a preprocessing function that applies the tokenizer to the text data in the dataset.\n",
        "def preprocess(reviews):\n",
        "  return tokenizer(reviews[\"text\"], truncation=True) #truncation=True ensures that the sequences are truncated to the maximum length supported by the model.\n",
        "\n",
        "tokenized_imdb_dataset = imdb_dataset.map(preprocess, batched=True) #batched=True processes the data in batches for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IVbBVqphjgAX"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding #To dynamically pad the sequences to the maximum length in the batch\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\") #Creating a data collator that will pad the sequences and return TensorFlow tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VyA3ePXjkOWC"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6NEBMogKkoPG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "  return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HbvW9aD6lfDr"
      },
      "outputs": [],
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldU-JbFJly9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\samir\\Desktop\\learning\\NLP\\LLM_projects\\LLM\\Sentiment_analysis\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import create_optimizer #To create an optimizer with a learning rate schedule like Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 5\n",
        "subset_size = 1024 # Using a subset for faster training\n",
        "\n",
        "steps_per_epoch = subset_size // batch_size\n",
        "total_train_steps = int(steps_per_epoch * num_epochs)\n",
        "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps) #num_warmup_steps=0 means no warmup phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55Dp9oYAm3WM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\samir\\Desktop\\learning\\NLP\\LLM_projects\\LLM\\Sentiment_analysis\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained DistilBERT model for sequence classification, we don't need to create the model from scratch.\n",
        "from transformers import TFAutoModelForSequenceClassification # The task is doing the sequence(reviews) to classification(sentiment label: positive/negative)\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "                                                     from_pt=True,\n",
        "                                                     num_labels=2,\n",
        "                                                     id2label=id2label,\n",
        "                                                     label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsTveLvCnd72"
      },
      "outputs": [],
      "source": [
        "shuffled_train_dataset = tokenized_imdb_dataset[\"train\"].shuffle(seed=42)\n",
        "shuffled_validation_dataset = tokenized_imdb_dataset[\"test\"].shuffle(seed=42)\n",
        "\n",
        "# shuffled_train_dataset is a dataset that is known to Hugging Face transformers library, so we can use the prepare_tf_dataset method to create TensorFlow datasets.\n",
        "\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    shuffled_train_dataset.select(range(subset_size)),\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "tf_validation_set = model.prepare_tf_dataset(\n",
        "    shuffled_validation_dataset.select(range(subset_size)),\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PHkBnPKaolIX"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JjAuBhUjoqQe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66362880  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66955010 (255.41 MB)\n",
            "Trainable params: 66955010 (255.41 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "th6HOhBsorcl"
      },
      "outputs": [],
      "source": [
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "\n",
        "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dcCVyqUSpRTl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:From c:\\Users\\samir\\Desktop\\learning\\NLP\\LLM_projects\\LLM\\Sentiment_analysis\\venv\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "64/64 [==============================] - 1081s 17s/step - loss: 0.2987 - val_loss: 0.2761 - accuracy: 0.8877\n",
            "Epoch 2/5\n",
            "64/64 [==============================] - 1075s 17s/step - loss: 0.1546 - val_loss: 0.3069 - accuracy: 0.8828\n",
            "Epoch 3/5\n",
            "64/64 [==============================] - 1070s 17s/step - loss: 0.0857 - val_loss: 0.3837 - accuracy: 0.8760\n",
            "Epoch 4/5\n",
            "64/64 [==============================] - 1057s 17s/step - loss: 0.0573 - val_loss: 0.3614 - accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "64/64 [==============================] - 1054s 17s/step - loss: 0.0409 - val_loss: 0.3706 - accuracy: 0.8867\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x20fc81d82f0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_epochs, callbacks=[metric_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-C36J0bwsUYp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: NEGATIVE\n"
          ]
        }
      ],
      "source": [
        "text = \"Saw an early screening of this film at the Tilton Square Theatre in New Jersey, and I was completely blown away. From the opening scene all the way until the credits I never felt bored, which is impressive for a 2 hour and 45 minute film.\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"tf\")\n",
        "logits = model(**inputs).logits\n",
        "predicted_class_idx = tf.math.argmax(logits, axis=-1)[0]\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx.numpy()])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
